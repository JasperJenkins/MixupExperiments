{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NoisyMixup.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fTPVxh1_x_wZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "if path.exists('/opt/bin/nvidia-smi'):\n",
        "  !pip install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl torchvision\n",
        "  !pip install dotted pyfastnoisesimd tqdm Pillow==4.0.0 PIL image\n",
        "  !wget -nc https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest/download/warwick_qu_dataset_released_2016_07_08.zip -O warick.zip\n",
        "  !unzip -q -o warick.zip\n",
        "  !mv 'Warwick QU Dataset (Released 2016_07_08)' warick_data\n",
        "else:\n",
        "  print('Select GPU backend')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q6PDpJcdjHag",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as F_img\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from dotted.collection import DottedDict\n",
        "from pyfastnoisesimd import generate\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "from random import uniform, randint\n",
        "import tqdm\n",
        "\n",
        "def conv(in_c, out_c):\n",
        "  return nn.Sequential(\n",
        "    nn.Conv2d(in_c, out_c, 3, padding=1, bias=False),\n",
        "    nn.ELU(inplace=True),\n",
        "    nn.BatchNorm2d(out_c),\n",
        "    nn.Conv2d(out_c, out_c, 3, padding=1, bias=False),\n",
        "    nn.ELU(inplace=True),\n",
        "    nn.BatchNorm2d(out_c),\n",
        "  )\n",
        "\n",
        "class UNet512(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UNet512, self).__init__()\n",
        "    self.down1 = conv(  3,  16) # (  3, 512, 512) --> ( 16, 512, 512)\n",
        "    self.down2 = conv( 16,  32) # ( 16, 256, 256) --> ( 32, 256, 256)\n",
        "    self.down3 = conv( 32,  64) # ( 32, 128, 128) --> ( 64, 128, 128)\n",
        "    self.down4 = conv( 64, 128) # ( 64,  64,  64) --> (128,  64,  64)\n",
        "    self.down5 = conv(128, 256) # (128,  32,  32) --> (256,  32,  32)\n",
        "    self.down6 = conv(256, 512) # (256,  16,  16) --> (512,  16,  16)\n",
        "    self.up1   = conv(768, 256) # (768,  32,  32) --> (256,  32,  32)\n",
        "    self.up2   = conv(384, 128) # (384,  64,  64) --> (128,  64,  64)\n",
        "    self.up3   = conv(192,  64) # (192, 128, 128) --> ( 64, 128, 128)\n",
        "    self.up4   = conv( 96,  32) # ( 32, 256, 256) --> ( 16, 256, 256)\n",
        "    self.up5   = conv( 48,  16) # ( 32, 512, 512) --> ( 16, 512, 512)\n",
        "    self.tail  = nn.Conv2d(16, 1, 1)\n",
        "    self.downpool = nn.MaxPool2d(kernel_size=2)\n",
        "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_down_512 = self.down1(x)\n",
        "    x_down_256 = self.down2(self.downpool(x_down_512))\n",
        "    x_down_128 = self.down3(self.downpool(x_down_256))\n",
        "    x_down_64  = self.down4(self.downpool(x_down_128))\n",
        "    x_down_32  = self.down5(self.downpool(x_down_64))\n",
        "    x_down_16  = self.down6(self.downpool(x_down_32))\n",
        "    x_up = self.up1(torch.cat([self.upsample(x_down_16), x_down_32], dim=1))\n",
        "    x_up = self.up2(torch.cat([self.upsample(x_up), x_down_64], dim=1))\n",
        "    x_up = self.up3(torch.cat([self.upsample(x_up), x_down_128], dim=1))\n",
        "    x_up = self.up4(torch.cat([self.upsample(x_up), x_down_256], dim=1))\n",
        "    x_up = self.up5(torch.cat([self.upsample(x_up), x_down_512], dim=1))\n",
        "    return self.tail(x_up)\n",
        "  \n",
        "class UNet256(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UNet256, self).__init__()\n",
        "    self.down1 = conv(  3,  16) # (  3, 256, 256) --> ( 16, 256, 256)\n",
        "    self.down2 = conv( 16,  32) # ( 16, 128, 128) --> ( 32, 128, 128)\n",
        "    self.down3 = conv( 32,  64) # ( 32,  64,  64) --> ( 64,  64,  64)\n",
        "    self.down4 = conv( 64, 128) # ( 64,  64,  64) --> (128,  32,  32)\n",
        "    self.down5 = conv(128, 256) # (128,  16,  16) --> (256,  16,  16)\n",
        "    self.down6 = conv(256, 512) # (256,   8,   8) --> (512,   8,   8)\n",
        "    self.up1   = conv(768, 256) # (768,  16,  16) --> (256,  16,  16)\n",
        "    self.up2   = conv(384, 128) # (384,  32,  32) --> (128,  32,  32)\n",
        "    self.up3   = conv(192,  64) # (192,  64,  64) --> ( 64,  64,  64)\n",
        "    self.up4   = conv( 96,  32) # ( 32, 128, 128) --> ( 16, 128, 128)\n",
        "    self.up5   = conv( 48,  16) # ( 32, 256, 256) --> ( 16, 256, 256)\n",
        "    self.tail  = nn.Conv2d(16, 1, 1)\n",
        "    self.downpool = nn.MaxPool2d(kernel_size=2)\n",
        "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_down_512 = self.down1(x)\n",
        "    x_down_256 = self.down2(self.downpool(x_down_512))\n",
        "    x_down_128 = self.down3(self.downpool(x_down_256))\n",
        "    x_down_64  = self.down4(self.downpool(x_down_128))\n",
        "    x_down_32  = self.down5(self.downpool(x_down_64))\n",
        "    x_down_16  = self.down6(self.downpool(x_down_32))\n",
        "    x_up = self.up1(torch.cat([self.upsample(x_down_16), x_down_32], dim=1))\n",
        "    x_up = self.up2(torch.cat([self.upsample(x_up), x_down_64], dim=1))\n",
        "    x_up = self.up3(torch.cat([self.upsample(x_up), x_down_128], dim=1))\n",
        "    x_up = self.up4(torch.cat([self.upsample(x_up), x_down_256], dim=1))\n",
        "    x_up = self.up5(torch.cat([self.upsample(x_up), x_down_512], dim=1))\n",
        "    return self.tail(x_up)\n",
        "\n",
        "class WarickData(Dataset):\n",
        "  def __init__(self, img_glob, img_size=256):\n",
        "    files = []\n",
        "    for file in glob(img_glob):\n",
        "      path_split = file.split('/')\n",
        "      post_split = path_split[-1].split('_')\n",
        "      if len(post_split) == 3:\n",
        "        img_file = f'{path_split[0]}/{post_split[0]}_{post_split[1]}.bmp'\n",
        "        mask_file = f'{path_split[0]}/{post_split[0]}_{post_split[1]}_anno.bmp'\n",
        "        files.append((img_file, mask_file))\n",
        "    self.files = files\n",
        "    self.img_mean, self.img_std = [200.248, 131.253, 199.778], [41.787, 62.667, 32.977]\n",
        "    self.mask_mean, self.mask_std = 2.512, 4.168\n",
        "    self.img_size = img_size\n",
        "\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(self.files[idx][0]) \n",
        "    img_mask = F_img.to_grayscale(Image.open(self.files[idx][1]))\n",
        "    \n",
        "    if random.random() > .5:\n",
        "      img = F_img.hflip(img)\n",
        "      img_mask = F_img.hflip(img_mask)\n",
        "    if random.random() > .5:\n",
        "      img = F_img.vflip(img)\n",
        "      img_mask = F_img.vflip(img_mask)\n",
        "    \n",
        "    img = F_img.resize(img, [self.img_size, self.img_size])\n",
        "    img_mask = F_img.resize(img_mask, [self.img_size, self.img_size])\n",
        "    return F_img.normalize(F_img.to_tensor(img), self.img_mean, self.img_std), F_img.to_tensor(img_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CTyXVICLyyi6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def plot_metrics(metric_logs, ylim=None):\n",
        "  for metric_log in metric_logs:\n",
        "    plt.plot(metric_log['epoch'], metric_log['metric'], label=metric_log['label'])\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Metric')\n",
        "  if ylim is not None: plt.ylim(*ylim)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def dice(pred, targs):\n",
        "    pred = (pred>0).float()\n",
        "    return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
        "  \n",
        "def dice_loss(input, target):\n",
        "    smooth = 1.\n",
        "\n",
        "    iflat = input.view(-1)\n",
        "    tflat = target.view(-1)\n",
        "    intersection = (iflat * tflat).sum()\n",
        "\n",
        "    return 1.0 - (((2. * intersection + smooth) /\n",
        "              (iflat.sum() + tflat.sum() + smooth)))\n",
        "  \n",
        "def train(model, epochs=1):\n",
        "  for e in tqdm.trange(epochs, desc='epochs'):\n",
        "    metric = 0\n",
        "    samples_seen = 0\n",
        "    model.net.train()\n",
        "    for img, mask in model.loader:\n",
        "      model.img_cuda.copy_(img)\n",
        "      del img\n",
        "      model.mask_cuda.copy_(mask)\n",
        "      del mask\n",
        "      model.optimizer.zero_grad()\n",
        "      prediction = model.net(model.img_cuda)\n",
        "      loss = dice_loss(F.sigmoid(prediction), model.mask_cuda) * 100\n",
        "      metric += loss.item()\n",
        "      samples_seen += model.batch_size\n",
        "      loss.backward()\n",
        "      model.optimizer.step()\n",
        "      model.scheduler.step()\n",
        "    model.train_metric_log['epoch'].append(model.epochs_trained)\n",
        "    model.train_metric_log['metric'].append(metric / samples_seen)\n",
        "    \n",
        "    if model.epochs_trained % model.eval_test == 0:\n",
        "      metric = 0\n",
        "      samples_seen = 0\n",
        "      model.net.eval()\n",
        "      with torch.no_grad():\n",
        "        for img, mask in model.loader_test:\n",
        "          model.img_cuda.copy_(img)\n",
        "          del img\n",
        "          model.mask_cuda.copy_(mask)\n",
        "          del mask\n",
        "          prediction = model.net(model.img_cuda)\n",
        "          loss = dice_loss(F.sigmoid(prediction), model.mask_cuda) * 100\n",
        "          metric += loss.item()\n",
        "          samples_seen += model.batch_size\n",
        "      model.test_metric_log['epoch'].append(model.epochs_trained)\n",
        "      model.test_metric_log['metric'].append(metric / samples_seen)\n",
        "    model.epochs_trained += 1\n",
        "\n",
        "def generate_noise():\n",
        "  return generate(size=[1, 256, 256], noiseType='Perlin',\n",
        "                  freq=uniform(.001, .05), seed=randint(0, 100000))[0]\n",
        "    \n",
        "def interpolate(a, b, f):\n",
        "  return (a * (1.0 - f)) + (b * f)\n",
        "    \n",
        "def train_mixup(model, epochs=1):\n",
        "  for e in tqdm.trange(epochs, desc='epochs'):\n",
        "    metric = 0\n",
        "    samples_seen = 0\n",
        "    model.net.train()\n",
        "    for (img_a, mask_a), (img_b, mask_b) in zip(model.loader, model.loader_other):\n",
        "      mixup_lerp = random.random()\n",
        "      model.img_cuda.copy_(interpolate(img_a, img_b, mixup_lerp))\n",
        "      del img_a; del img_b\n",
        "      model.mask_cuda.copy_(interpolate(mask_a, mask_b, mixup_lerp))\n",
        "      del mask_a; del mask_b\n",
        "      model.optimizer.zero_grad()\n",
        "      prediction = model.net(model.img_cuda)\n",
        "      loss = dice_loss(F.sigmoid(prediction), model.mask_cuda) * 100\n",
        "      metric += loss.item()\n",
        "      samples_seen += model.batch_size\n",
        "      loss.backward()\n",
        "      model.optimizer.step()\n",
        "      model.scheduler.step()\n",
        "    model.train_metric_log['epoch'].append(model.epochs_trained)\n",
        "    model.train_metric_log['metric'].append(metric / samples_seen)\n",
        "    \n",
        "    if model.epochs_trained % model.eval_test == 0:\n",
        "      metric = 0\n",
        "      samples_seen = 0\n",
        "      model.net.eval()\n",
        "      with torch.no_grad():\n",
        "        for img, mask in model.loader_test:\n",
        "          model.img_cuda.copy_(img)\n",
        "          del img\n",
        "          model.mask_cuda.copy_(mask)\n",
        "          del mask\n",
        "          prediction = model.net(model.img_cuda)\n",
        "          loss = dice_loss(F.sigmoid(prediction), model.mask_cuda) * 100\n",
        "          metric += loss.item()\n",
        "          samples_seen += model.batch_size\n",
        "      model.test_metric_log['epoch'].append(model.epochs_trained)\n",
        "      model.test_metric_log['metric'].append(metric / samples_seen)\n",
        "    model.epochs_trained += 1\n",
        "\n",
        "def train_noisy_mixup(model, epochs=1):\n",
        "  for e in tqdm.trange(epochs, desc='epochs'):\n",
        "    metric = 0\n",
        "    samples_seen = 0\n",
        "    model.net.train()\n",
        "    for (img_a, mask_a), (img_b, mask_b) in zip(model.loader, model.loader_other):\n",
        "      mixup_lerp = torch.tensor(generate_noise())\n",
        "      model.img_cuda.copy_(interpolate(img_a, img_b, mixup_lerp))\n",
        "      del img_a; del img_b\n",
        "      model.mask_cuda.copy_(interpolate(mask_a, mask_b, mixup_lerp))\n",
        "      del mask_a; del mask_b\n",
        "      model.optimizer.zero_grad()\n",
        "      prediction = model.net(model.img_cuda)\n",
        "      loss = dice_loss(F.sigmoid(prediction), model.mask_cuda) * 100\n",
        "      metric += loss.item()\n",
        "      samples_seen += model.batch_size\n",
        "      loss.backward()\n",
        "      model.optimizer.step()\n",
        "      model.scheduler.step()\n",
        "    model.train_metric_log['epoch'].append(model.epochs_trained)\n",
        "    model.train_metric_log['metric'].append(metric / samples_seen)\n",
        "    \n",
        "    if model.epochs_trained % model.eval_test == 0:\n",
        "      metric = 0\n",
        "      samples_seen = 0\n",
        "      model.net.eval()\n",
        "      with torch.no_grad():\n",
        "        for img, mask in model.loader_test:\n",
        "          model.img_cuda.copy_(img)\n",
        "          del img\n",
        "          model.mask_cuda.copy_(mask)\n",
        "          del mask\n",
        "          prediction = model.net(model.img_cuda)\n",
        "          loss = dice_loss(F.sigmoid(prediction), model.mask_cuda) * 100\n",
        "          metric += loss.item()\n",
        "          samples_seen += model.batch_size\n",
        "      model.test_metric_log['epoch'].append(model.epochs_trained)\n",
        "      model.test_metric_log['metric'].append(metric / samples_seen)\n",
        "    model.epochs_trained += 1\n",
        "    \n",
        "def pickle_history(model, file):\n",
        "  pickle.dump({\n",
        "    'train_metric_log': model['train_metric_log'],\n",
        "    'test_metric_log': model['test_metric_log'],\n",
        "  }, open(file, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qolbfYS-Udp-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_data, test_data = WarickData('warick_data/train*anno.bmp'), WarickData('warick_data/test*anno.bmp')\n",
        "\n",
        "model = DottedDict()\n",
        "model['seed'] = 53\n",
        "set_seed(model.seed)\n",
        "model['batch_size'] = 21\n",
        "model['net'] = UNet256().cuda()\n",
        "model['optimizer'] = optim.Adam(model.net.parameters(), lr=0.05)\n",
        "model['scheduler'] = optim.lr_scheduler.CosineAnnealingLR(model.optimizer, len(train_data) * 1000 / model.batch_size, .00005)\n",
        "model['loader'] = DataLoader(train_data, model.batch_size, True, drop_last=True, pin_memory=True)\n",
        "model['loader_other'] = DataLoader(train_data, model.batch_size, True, drop_last=True, pin_memory=True)\n",
        "model['loader_test'] = DataLoader(test_data, model.batch_size, False, drop_last=True, pin_memory=True)\n",
        "model['img_cuda'] = torch.empty([model.batch_size, 3, 256, 256]).cuda()\n",
        "model['mask_cuda'] = torch.empty([model.batch_size, 1, 256, 256]).cuda()\n",
        "model['epochs_trained'] = 0\n",
        "model['eval_test'] = 1\n",
        "model['train_metric_log'] = {'label':'Train Loss', 'epoch':[], 'metric':[]}\n",
        "model['test_metric_log'] = {'label':'Test Loss', 'epoch':[], 'metric':[]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5YC3n8LRVKsC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train(model, 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLP4ysUt_crl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model.net.eval()\n",
        "  batch = next(iter(model.loader))\n",
        "\n",
        "  predicted_masks = model.net(batch[0].cuda()).cpu()\n",
        "  img = (np.moveaxis(batch[0][0].numpy(), 0, 2) * \n",
        "         np.array([41.787, 62.667, 32.977]) + \n",
        "         np.array([200.248, 131.253, 199.778]))\n",
        "  mask, mask_prediction = batch[1][0].numpy(), predicted_masks[0].numpy()\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.imshow(mask[0], cmap='inferno', alpha=.5)\n",
        "plt.show()\n",
        "plt.imshow(img)\n",
        "plt.imshow(mask_prediction[0], cmap='inferno', alpha=.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t9O8xlfDanFC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plot_metrics([model.train_metric_log, model.test_metric_log], (0, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4Q_6Tvucy-S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "hist_file = f'standard_seed53_res256_epoch{model.epochs_trained}.hist'\n",
        "\n",
        "pickle_history(model.to_python(), hist_file)\n",
        "files.download(hist_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y5FBZIwdN-PT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model_file = f'standard_seed53_res256_epoch{model.epochs_trained}.model'\n",
        "\n",
        "torch.save(model.to_python(), model_file)\n",
        "files.download(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}