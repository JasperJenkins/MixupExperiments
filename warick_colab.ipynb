{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NoisyMixup.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fTPVxh1_x_wZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "if path.exists('/opt/bin/nvidia-smi'):\n",
        "  !pip install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl torchvision\n",
        "  !pip install dotted pyfastnoisesimd tqdm Pillow==4.0.0 PIL image\n",
        "  !wget -nc https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest/download/warwick_qu_dataset_released_2016_07_08.zip -O warick.zip\n",
        "  !unzip -q -o warick.zip\n",
        "  !mv 'Warwick QU Dataset (Released 2016_07_08)' warick_data\n",
        "else:\n",
        "  print('Select GPU backend')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CTyXVICLyyi6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from dotted.collection import DottedDict\n",
        "from pyfastnoisesimd import generate\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "from random import uniform, randint\n",
        "import tqdm\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def conv(in_c, out_c):\n",
        "  return nn.Sequential(\n",
        "    nn.Conv2d(in_c, out_c, 3, padding=1, bias=False),\n",
        "    nn.ELU(inplace=True),\n",
        "    nn.BatchNorm2d(out_c),\n",
        "    nn.Conv2d(out_c, out_c, 3, padding=1, bias=False),\n",
        "    nn.ELU(inplace=True),\n",
        "    nn.BatchNorm2d(out_c),\n",
        "  )\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(UNet, self).__init__()\n",
        "    self.down1 = conv(  3,  16) # (  3, 512, 512) --> ( 16, 512, 512)\n",
        "    self.down2 = conv( 16,  32) # ( 16, 256, 256) --> ( 32, 256, 256)\n",
        "    self.down3 = conv( 32,  64) # ( 32, 128, 128) --> ( 64, 128, 128)\n",
        "    self.down4 = conv( 64, 128) # ( 64,  64,  64) --> (128,  64,  64)\n",
        "    self.down5 = conv(128, 256) # (128,  32,  32) --> (256,  32,  32)\n",
        "    self.down6 = conv(256, 512) # (256,  16,  16) --> (512,  16,  16)\n",
        "    self.up1   = conv(768, 256) # (768,  32,  32) --> (256,  32,  32)\n",
        "    self.up2   = conv(384, 128) # (384,  64,  64) --> (128,  64,  64)\n",
        "    self.up3   = conv(192,  64) # (192, 128, 128) --> ( 64, 128, 128)\n",
        "    self.up4   = conv( 96,  32) # ( 32, 256, 256) --> ( 16, 256, 256)\n",
        "    self.up5   = conv( 48,  16) # ( 32, 512, 512) --> ( 16, 512, 512)\n",
        "    self.tail  = nn.Conv2d(16, 1, 1)\n",
        "    self.downpool = nn.MaxPool2d(kernel_size=2)\n",
        "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_down_512 = self.down1(x)\n",
        "    x_down_256 = self.down2(self.downpool(x_down_512))\n",
        "    x_down_128 = self.down3(self.downpool(x_down_256))\n",
        "    x_down_64  = self.down4(self.downpool(x_down_128))\n",
        "    x_down_32  = self.down5(self.downpool(x_down_64))\n",
        "    x_down_16  = self.down6(self.downpool(x_down_32))\n",
        "    x_up = self.up1(torch.cat([self.upsample(x_down_16), x_down_32], dim=1))\n",
        "    x_up = self.up2(torch.cat([self.upsample(x_up), x_down_64], dim=1))\n",
        "    x_up = self.up3(torch.cat([self.upsample(x_up), x_down_128], dim=1))\n",
        "    x_up = self.up4(torch.cat([self.upsample(x_up), x_down_256],  dim=1))\n",
        "    x_up = self.up5(torch.cat([self.upsample(x_up), x_down_512]))\n",
        "    return self.tail(x_up)\n",
        "\n",
        "def plot_metrics(metric_logs):\n",
        "  for metric_log in metric_logs:\n",
        "    plt.plot(metric_log['epoch'], metric_log['metric'], label=metric_log['label'])\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Metric')\n",
        "  plt.ylim(0, 5)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def dice(pred, targs):\n",
        "    pred = (pred>0).float()\n",
        "    return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
        "  \n",
        "def dice_loss(input, target):\n",
        "    smooth = 1.\n",
        "\n",
        "    iflat = input.view(-1)\n",
        "    tflat = target.view(-1)\n",
        "    intersection = (iflat * tflat).sum()\n",
        "\n",
        "    return 1.0 - (((2. * intersection + smooth) /\n",
        "              (iflat.sum() + tflat.sum() + smooth)))\n",
        "  \n",
        "def train(model, epochs=1):\n",
        "  for e in tqdm.trange(epochs, desc='epochs'):\n",
        "    metric = 0\n",
        "    samples_seen = 0\n",
        "    model.net.train()\n",
        "    for img, mask in model.loader:\n",
        "      model.img_cuda.copy_(img)\n",
        "      del img\n",
        "      model.mask_cuda.copy_(mask)\n",
        "      del mask\n",
        "      model.optimizer.zero_grad()\n",
        "      prediction = model.net(model.img_cuda)\n",
        "      loss = (F.binary_cross_entropy_with_logits(prediction, model.mask_cuda) +\n",
        "              dice_loss(F.sigmoid(prediction), model.mask_cuda)) * 100\n",
        "      metric += loss.item()\n",
        "      samples_seen += model.batch_size\n",
        "      loss.backward()\n",
        "      model.optimizer.step()\n",
        "      model.scheduler.step()\n",
        "    model.train_metric_log['epoch'].append(model.epochs_trained)\n",
        "    model.train_metric_log['metric'].append(metric / samples_seen)\n",
        "    \n",
        "    if model.epochs_trained % model.eval_test == 0:\n",
        "      metric = 0\n",
        "      samples_seen = 0\n",
        "      model.net.eval()\n",
        "      with torch.no_grad():\n",
        "        for img, mask in model.loader_test:\n",
        "          model.img_cuda.copy_(img)\n",
        "          del img\n",
        "          model.mask_cuda.copy_(mask)\n",
        "          del mask\n",
        "          prediction = model.net(model.img_cuda)\n",
        "          loss = (F.binary_cross_entropy_with_logits(prediction, model.mask_cuda) +\n",
        "                  dice_loss(F.sigmoid(prediction), model.mask_cuda)) * 100\n",
        "          metric += loss.item()\n",
        "          samples_seen += model.batch_size\n",
        "      model.test_metric_log['epoch'].append(model.epochs_trained)\n",
        "      model.test_metric_log['metric'].append(metric / samples_seen)\n",
        "    model.epochs_trained += 1\n",
        "\n",
        "def generate_noise():\n",
        "  return fns.generate(size=[1, 128, 128], noiseType='Perlin',\n",
        "                      freq=uniform(.01, .075), seed=randint(0, 100000))[0]\n",
        "    \n",
        "def interpolate(a, b, f):\n",
        "  return (a * (1.0 - f)) + (b * f)\n",
        "    \n",
        "def train_mixup(model, epochs=1):\n",
        "  for e in tqdm.trange(epochs, desc='epochs'):\n",
        "    metric = 0\n",
        "    samples_seen = 0\n",
        "    model.net.train()\n",
        "    for (img_a, mask_a), (img_b, mask_b) in zip(model.loader, model.loader_other):\n",
        "      mixup_lerp = random.random()\n",
        "      model.img_cuda.copy_(interpolate(img_a, img_b, mixup_lerp))\n",
        "      del img_a; del img_b\n",
        "      model.mask_cuda.copy_(interpolate(mask_a, mask_b, mixup_lerp))\n",
        "      del mask_a; del mask_b\n",
        "      model.optimizer.zero_grad()\n",
        "      prediction = model.net(model.img_cuda)\n",
        "      loss = (F.binary_cross_entropy_with_logits(prediction, model.mask_cuda) +\n",
        "              dice_loss(F.sigmoid(prediction), model.mask_cuda)) * 100\n",
        "      metric += loss.item()\n",
        "      samples_seen += model.batch_size\n",
        "      loss.backward()\n",
        "      model.optimizer.step()\n",
        "      model.scheduler.step()\n",
        "    model.train_metric_log['epoch'].append(model.epochs_trained)\n",
        "    model.train_metric_log['metric'].append(metric / samples_seen)\n",
        "    \n",
        "    if model.epochs_trained % model.eval_test == 0:\n",
        "      metric = 0\n",
        "      samples_seen = 0\n",
        "      model.net.eval()\n",
        "      with torch.no_grad():\n",
        "        for img, mask in model.loader_test:\n",
        "          model.img_cuda.copy_(img)\n",
        "          del img\n",
        "          model.mask_cuda.copy_(mask)\n",
        "          del mask\n",
        "          prediction = model.net(model.img_cuda)\n",
        "          loss = (F.binary_cross_entropy_with_logits(prediction, model.mask_cuda) +\n",
        "                  dice_loss(F.sigmoid(prediction), model.mask_cuda)) * 100\n",
        "          metric += loss.item()\n",
        "          samples_seen += model.batch_size\n",
        "      model.test_metric_log['epoch'].append(model.epochs_trained)\n",
        "      model.test_metric_log['metric'].append(metric / samples_seen)\n",
        "    model.epochs_trained += 1\n",
        "  \n",
        "def weights_init(m):\n",
        "  with torch.no_grad():\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "      torch.nn.init.xavier_uniform_(m.weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fvl4IC6bIqoK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "img, mask = torch.rand([1, 3, 512, 512]), torch.rand([1, 1, 512, 512])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M3OaVtZoEs1M",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "img_mean, img_std = [200.248, 131.253, 199.778], [41.787, 62.667, 32.977]\n",
        "mask_mean, mask_std = 2.512, 4.168"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mWHBLjM900rQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train_img_files = glob('warick_data/train*[!anno].bmp')\n",
        "train_mask_files = glob('warick_data/train*anno.bmp')\n",
        "test_img_files = glob('warick_data/test*[!anno].bmp')\n",
        "test_mask_files = glob('warick_data/test*anno.bmp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9QGk_4Fv_E6w",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "img_sizes = set(Image.open(file).size for file in files)\n",
        "\n",
        "sizes_a, sizes_b = [], []\n",
        "for size_a, size_b in img_sizes:\n",
        "  sizes_a.append(size_a); sizes_b.append(size_b)\n",
        "  \n",
        "sizes_a, sizes_b = np.array(sizes_a), np.array(sizes_b)\n",
        "\n",
        "sizes_a.mean(), sizes_a.std(), sizes_b.mean(), sizes_b.std()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}